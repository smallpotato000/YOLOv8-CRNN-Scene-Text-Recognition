{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Common Training/Validation Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "from IPython.display import Image\n",
    "import cv2\n",
    "import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Download"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Roboflow Universe Dataset  \n",
    "Download the dataset using Roboflow API through this [link](https://universe.roboflow.com/text-detector/text-detector-2/dataset/2).  \n",
    "The dataset will be stored at `datasets/{project-name}`. Roboflow provides ready to use dataset complete with splits and annotations.  \n",
    "Below is example code to download dataset to local."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from roboflow import Roboflow\n",
    "\n",
    "# rf = Roboflow(api_key=\"0n9ducziAw8kQIGHk25M\")\n",
    "# project = rf.workspace(\"text-detector\").project(\"text-detector-2\")\n",
    "# dataset = project.version(2).download(\"yolov8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Custom Dataset  \n",
    "For the custom dataset, I will use the Kaggle [TextOCR](https://www.kaggle.com/datasets/robikscube/textocr-text-extraction-from-images-dataset) dataset.  \n",
    "Download the dataset (roughly 7 GB) and store it in the `datasets` directory.  \n",
    "Make sure to organize the files and folders accordingly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create custom YOLOv8 data generator using the `yolov8datagen.py` script. Use the following script to generate custom dataset with YOLOv8 from the downloaded Kaggle dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "python yolov8datagen.py --source_dir --dest_dir --total_images --density --split\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Refer to `yolov8datagen.py` for full script arguments and outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check GPU Compatibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Apr 23 12:28:10 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 570.124.06             Driver Version: 570.124.06     CUDA Version: 12.8     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA RTX A4000               Off |   00000000:52:00.0  On |                  Off |\n",
      "| 67%   85C    P2            127W /  140W |   10657MiB /  16376MiB |     80%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A           47684      C   ...c/gnome-remote-desktop-daemon        158MiB |\n",
      "|    0   N/A  N/A           47739      G   /usr/bin/gnome-shell                    222MiB |\n",
      "|    0   N/A  N/A           47866      G   ...c/gnome-remote-desktop-daemon          3MiB |\n",
      "|    0   N/A  N/A           48059      G   /usr/bin/Xwayland                         3MiB |\n",
      "|    0   N/A  N/A          599436      C   ...3/envs/ultralytics/bin/python      10218MiB |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training loop of YOLOv8 using Ultralytics API is fairly simple:\n",
    "\n",
    "1. Initialize the YOLO model and its specified architecture. (`yolov8n.pt` refers to the nano-sized model; refer to [this link](https://docs.ultralytics.com/modes/) for more information.)  \n",
    "   - the code will download the pretrained weights if not already existed.\n",
    "\n",
    "2. Use the `train` method with the following arguments:\n",
    "   - `data`: This should point to our source, which should be a `data.yaml` file stored in the `datasets` directory.\n",
    "\n",
    "3. Set hyperparameters and device (use 0 for CUDA) and save the results.\n",
    "   - setting the `optimizer` arguments to `auto` (the default) will let YOLO decide the best value for some hyperparameters such as learning rate and momentum.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics 8.3.100 🚀 Python-3.10.13 torch-2.5.1 CUDA:0 (NVIDIA RTX A4000, 15977MiB)\n",
      "\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=detect, mode=train, model=yolov8n.pt, data=/home/akiko/work/YOLOv8-CRNN-Scene-Text-Recognition/datasets/ctw_and_textocr/data.yaml, epochs=20, time=None, patience=100, batch=8, imgsz=640, save=True, save_period=-1, cache=False, device=0, workers=8, project=None, name=train2, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=True, opset=None, workspace=None, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, bgr=0.0, mosaic=1.0, mixup=0.0, copy_paste=0.0, copy_paste_mode=flip, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=runs/detect/train2\n",
      "Overriding model.yaml nc=80 with nc=1\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
      "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
      "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
      "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
      "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
      "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
      " 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
      " 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
      " 22        [15, 18, 21]  1    751507  ultralytics.nn.modules.head.Detect           [1, [64, 128, 256]]           \n",
      "Model summary: 129 layers, 3,011,043 parameters, 3,011,027 gradients\n",
      "\n",
      "Transferred 319/355 items from pretrained weights\n",
      "Freezing layer 'model.22.dfl.conv.weight'\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ✅\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /home/akiko/work/YOLOv8-CRNN-Scene-Text-Recognition/datasets/ctw_and_textocr/train/labels.cache... 28790 images, 0 backgrounds, 90 corrupt: 100%|██████████| 28790/28790 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /home/akiko/work/YOLOv8-CRNN-Scene-Text-Recognition/datasets/ctw_and_textocr/train/images/1000929.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0001]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /home/akiko/work/YOLOv8-CRNN-Scene-Text-Recognition/datasets/ctw_and_textocr/train/images/1004397.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0015]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /home/akiko/work/YOLOv8-CRNN-Scene-Text-Recognition/datasets/ctw_and_textocr/train/images/1004769.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [          1]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /home/akiko/work/YOLOv8-CRNN-Scene-Text-Recognition/datasets/ctw_and_textocr/train/images/1004788.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0001]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /home/akiko/work/YOLOv8-CRNN-Scene-Text-Recognition/datasets/ctw_and_textocr/train/images/1007254.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0001]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /home/akiko/work/YOLOv8-CRNN-Scene-Text-Recognition/datasets/ctw_and_textocr/train/images/1010267.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0008]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /home/akiko/work/YOLOv8-CRNN-Scene-Text-Recognition/datasets/ctw_and_textocr/train/images/1010535.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0018]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /home/akiko/work/YOLOv8-CRNN-Scene-Text-Recognition/datasets/ctw_and_textocr/train/images/1018118.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0696]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /home/akiko/work/YOLOv8-CRNN-Scene-Text-Recognition/datasets/ctw_and_textocr/train/images/1018812.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0006]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /home/akiko/work/YOLOv8-CRNN-Scene-Text-Recognition/datasets/ctw_and_textocr/train/images/1020749.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0001      1.0004]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /home/akiko/work/YOLOv8-CRNN-Scene-Text-Recognition/datasets/ctw_and_textocr/train/images/1023901.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0026]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /home/akiko/work/YOLOv8-CRNN-Scene-Text-Recognition/datasets/ctw_and_textocr/train/images/1025158.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [          1]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /home/akiko/work/YOLOv8-CRNN-Scene-Text-Recognition/datasets/ctw_and_textocr/train/images/1026825.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0055]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /home/akiko/work/YOLOv8-CRNN-Scene-Text-Recognition/datasets/ctw_and_textocr/train/images/1027648.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0002]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /home/akiko/work/YOLOv8-CRNN-Scene-Text-Recognition/datasets/ctw_and_textocr/train/images/1034057.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0022]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /home/akiko/work/YOLOv8-CRNN-Scene-Text-Recognition/datasets/ctw_and_textocr/train/images/1042497.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0028]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /home/akiko/work/YOLOv8-CRNN-Scene-Text-Recognition/datasets/ctw_and_textocr/train/images/1042581.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0001]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /home/akiko/work/YOLOv8-CRNN-Scene-Text-Recognition/datasets/ctw_and_textocr/train/images/1046298.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0003]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /home/akiko/work/YOLOv8-CRNN-Scene-Text-Recognition/datasets/ctw_and_textocr/train/images/1046314.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0001]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /home/akiko/work/YOLOv8-CRNN-Scene-Text-Recognition/datasets/ctw_and_textocr/train/images/2000412.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0027]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /home/akiko/work/YOLOv8-CRNN-Scene-Text-Recognition/datasets/ctw_and_textocr/train/images/2002097.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0017]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /home/akiko/work/YOLOv8-CRNN-Scene-Text-Recognition/datasets/ctw_and_textocr/train/images/2002202.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0004]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /home/akiko/work/YOLOv8-CRNN-Scene-Text-Recognition/datasets/ctw_and_textocr/train/images/2002263.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0012]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /home/akiko/work/YOLOv8-CRNN-Scene-Text-Recognition/datasets/ctw_and_textocr/train/images/2002694.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0012]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /home/akiko/work/YOLOv8-CRNN-Scene-Text-Recognition/datasets/ctw_and_textocr/train/images/2003178.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0252]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /home/akiko/work/YOLOv8-CRNN-Scene-Text-Recognition/datasets/ctw_and_textocr/train/images/2003449.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0005]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /home/akiko/work/YOLOv8-CRNN-Scene-Text-Recognition/datasets/ctw_and_textocr/train/images/2003453.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0006]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /home/akiko/work/YOLOv8-CRNN-Scene-Text-Recognition/datasets/ctw_and_textocr/train/images/2003673.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0088]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /home/akiko/work/YOLOv8-CRNN-Scene-Text-Recognition/datasets/ctw_and_textocr/train/images/2003678.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0006]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /home/akiko/work/YOLOv8-CRNN-Scene-Text-Recognition/datasets/ctw_and_textocr/train/images/2003691.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0005      1.0011]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /home/akiko/work/YOLOv8-CRNN-Scene-Text-Recognition/datasets/ctw_and_textocr/train/images/2003811.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0002]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /home/akiko/work/YOLOv8-CRNN-Scene-Text-Recognition/datasets/ctw_and_textocr/train/images/2004203.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0008]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /home/akiko/work/YOLOv8-CRNN-Scene-Text-Recognition/datasets/ctw_and_textocr/train/images/2008077.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0002]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /home/akiko/work/YOLOv8-CRNN-Scene-Text-Recognition/datasets/ctw_and_textocr/train/images/2008504.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0002]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /home/akiko/work/YOLOv8-CRNN-Scene-Text-Recognition/datasets/ctw_and_textocr/train/images/2008630.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0005      1.0025]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /home/akiko/work/YOLOv8-CRNN-Scene-Text-Recognition/datasets/ctw_and_textocr/train/images/2008762.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0015]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /home/akiko/work/YOLOv8-CRNN-Scene-Text-Recognition/datasets/ctw_and_textocr/train/images/2014806.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0003]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /home/akiko/work/YOLOv8-CRNN-Scene-Text-Recognition/datasets/ctw_and_textocr/train/images/2014876.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0001      1.0001]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /home/akiko/work/YOLOv8-CRNN-Scene-Text-Recognition/datasets/ctw_and_textocr/train/images/2015080.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0001]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /home/akiko/work/YOLOv8-CRNN-Scene-Text-Recognition/datasets/ctw_and_textocr/train/images/2015133.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [          1      1.0001      1.0003      1.0004      1.0006      1.0008      1.0009      1.0011]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /home/akiko/work/YOLOv8-CRNN-Scene-Text-Recognition/datasets/ctw_and_textocr/train/images/2015392.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0009]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /home/akiko/work/YOLOv8-CRNN-Scene-Text-Recognition/datasets/ctw_and_textocr/train/images/2015425.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0001      1.0011]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /home/akiko/work/YOLOv8-CRNN-Scene-Text-Recognition/datasets/ctw_and_textocr/train/images/2022306.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0001]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /home/akiko/work/YOLOv8-CRNN-Scene-Text-Recognition/datasets/ctw_and_textocr/train/images/2022314.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [          1]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /home/akiko/work/YOLOv8-CRNN-Scene-Text-Recognition/datasets/ctw_and_textocr/train/images/2024074.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0006]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /home/akiko/work/YOLOv8-CRNN-Scene-Text-Recognition/datasets/ctw_and_textocr/train/images/2025380.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0001]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /home/akiko/work/YOLOv8-CRNN-Scene-Text-Recognition/datasets/ctw_and_textocr/train/images/2033638.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0001]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /home/akiko/work/YOLOv8-CRNN-Scene-Text-Recognition/datasets/ctw_and_textocr/train/images/2034015.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0007]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /home/akiko/work/YOLOv8-CRNN-Scene-Text-Recognition/datasets/ctw_and_textocr/train/images/2041098.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0009]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /home/akiko/work/YOLOv8-CRNN-Scene-Text-Recognition/datasets/ctw_and_textocr/train/images/2041232.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0002]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /home/akiko/work/YOLOv8-CRNN-Scene-Text-Recognition/datasets/ctw_and_textocr/train/images/2042445.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0012]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /home/akiko/work/YOLOv8-CRNN-Scene-Text-Recognition/datasets/ctw_and_textocr/train/images/2042508.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0004]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /home/akiko/work/YOLOv8-CRNN-Scene-Text-Recognition/datasets/ctw_and_textocr/train/images/2042512.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0019]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /home/akiko/work/YOLOv8-CRNN-Scene-Text-Recognition/datasets/ctw_and_textocr/train/images/2042571.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0002]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /home/akiko/work/YOLOv8-CRNN-Scene-Text-Recognition/datasets/ctw_and_textocr/train/images/2043868.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0008]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /home/akiko/work/YOLOv8-CRNN-Scene-Text-Recognition/datasets/ctw_and_textocr/train/images/3000862.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [          1]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /home/akiko/work/YOLOv8-CRNN-Scene-Text-Recognition/datasets/ctw_and_textocr/train/images/3001536.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0005]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /home/akiko/work/YOLOv8-CRNN-Scene-Text-Recognition/datasets/ctw_and_textocr/train/images/3002692.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [          1]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /home/akiko/work/YOLOv8-CRNN-Scene-Text-Recognition/datasets/ctw_and_textocr/train/images/3004829.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0001]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /home/akiko/work/YOLOv8-CRNN-Scene-Text-Recognition/datasets/ctw_and_textocr/train/images/3004885.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0016]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /home/akiko/work/YOLOv8-CRNN-Scene-Text-Recognition/datasets/ctw_and_textocr/train/images/3005737.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0021]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /home/akiko/work/YOLOv8-CRNN-Scene-Text-Recognition/datasets/ctw_and_textocr/train/images/3014566.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0003]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /home/akiko/work/YOLOv8-CRNN-Scene-Text-Recognition/datasets/ctw_and_textocr/train/images/3018071.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0002]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /home/akiko/work/YOLOv8-CRNN-Scene-Text-Recognition/datasets/ctw_and_textocr/train/images/3018378.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0016]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /home/akiko/work/YOLOv8-CRNN-Scene-Text-Recognition/datasets/ctw_and_textocr/train/images/3021613.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [          1]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /home/akiko/work/YOLOv8-CRNN-Scene-Text-Recognition/datasets/ctw_and_textocr/train/images/3023849.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0009]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /home/akiko/work/YOLOv8-CRNN-Scene-Text-Recognition/datasets/ctw_and_textocr/train/images/3023892.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0014      1.0019]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /home/akiko/work/YOLOv8-CRNN-Scene-Text-Recognition/datasets/ctw_and_textocr/train/images/3023915.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0009]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /home/akiko/work/YOLOv8-CRNN-Scene-Text-Recognition/datasets/ctw_and_textocr/train/images/3023983.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0088]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /home/akiko/work/YOLOv8-CRNN-Scene-Text-Recognition/datasets/ctw_and_textocr/train/images/3025463.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0001]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /home/akiko/work/YOLOv8-CRNN-Scene-Text-Recognition/datasets/ctw_and_textocr/train/images/3025534.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0006]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /home/akiko/work/YOLOv8-CRNN-Scene-Text-Recognition/datasets/ctw_and_textocr/train/images/3025577.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0004]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /home/akiko/work/YOLOv8-CRNN-Scene-Text-Recognition/datasets/ctw_and_textocr/train/images/3025602.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0005]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /home/akiko/work/YOLOv8-CRNN-Scene-Text-Recognition/datasets/ctw_and_textocr/train/images/3025636.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [          1]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /home/akiko/work/YOLOv8-CRNN-Scene-Text-Recognition/datasets/ctw_and_textocr/train/images/3027100.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0008]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /home/akiko/work/YOLOv8-CRNN-Scene-Text-Recognition/datasets/ctw_and_textocr/train/images/3027469.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0025]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /home/akiko/work/YOLOv8-CRNN-Scene-Text-Recognition/datasets/ctw_and_textocr/train/images/3028304.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0003]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /home/akiko/work/YOLOv8-CRNN-Scene-Text-Recognition/datasets/ctw_and_textocr/train/images/3028606.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0012]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /home/akiko/work/YOLOv8-CRNN-Scene-Text-Recognition/datasets/ctw_and_textocr/train/images/3029319.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0001      1.0009]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /home/akiko/work/YOLOv8-CRNN-Scene-Text-Recognition/datasets/ctw_and_textocr/train/images/3029322.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0016      1.0004]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /home/akiko/work/YOLOv8-CRNN-Scene-Text-Recognition/datasets/ctw_and_textocr/train/images/3029440.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0004]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /home/akiko/work/YOLOv8-CRNN-Scene-Text-Recognition/datasets/ctw_and_textocr/train/images/3030089.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0001]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /home/akiko/work/YOLOv8-CRNN-Scene-Text-Recognition/datasets/ctw_and_textocr/train/images/3031910.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0003       1.001]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /home/akiko/work/YOLOv8-CRNN-Scene-Text-Recognition/datasets/ctw_and_textocr/train/images/3033223.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [          1      1.0002]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /home/akiko/work/YOLOv8-CRNN-Scene-Text-Recognition/datasets/ctw_and_textocr/train/images/3033963.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0008]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /home/akiko/work/YOLOv8-CRNN-Scene-Text-Recognition/datasets/ctw_and_textocr/train/images/3034013.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0002      1.0002      1.0002      1.0002      1.0002      1.0002      1.0002]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /home/akiko/work/YOLOv8-CRNN-Scene-Text-Recognition/datasets/ctw_and_textocr/train/images/3034017.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0026]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /home/akiko/work/YOLOv8-CRNN-Scene-Text-Recognition/datasets/ctw_and_textocr/train/images/3042868.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0001]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /home/akiko/work/YOLOv8-CRNN-Scene-Text-Recognition/datasets/ctw_and_textocr/train/images/3043800.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0018]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /home/akiko/work/YOLOv8-CRNN-Scene-Text-Recognition/datasets/ctw_and_textocr/train/images/3047781.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0003]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /home/akiko/work/YOLOv8-CRNN-Scene-Text-Recognition/datasets/ctw_and_textocr/val/labels.cache... 1847 images, 0 backgrounds, 5 corrupt: 100%|██████████| 1847/1847 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /home/akiko/work/YOLOv8-CRNN-Scene-Text-Recognition/datasets/ctw_and_textocr/val/images/1000963.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0001]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /home/akiko/work/YOLOv8-CRNN-Scene-Text-Recognition/datasets/ctw_and_textocr/val/images/2002701.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0012]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /home/akiko/work/YOLOv8-CRNN-Scene-Text-Recognition/datasets/ctw_and_textocr/val/images/3017992.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [      1.001]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /home/akiko/work/YOLOv8-CRNN-Scene-Text-Recognition/datasets/ctw_and_textocr/val/images/3031111.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0003]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /home/akiko/work/YOLOv8-CRNN-Scene-Text-Recognition/datasets/ctw_and_textocr/val/images/3031161.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0007]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting labels to runs/detect/train2/labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.002, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n",
      "Image sizes 640 train, 640 val\n",
      "Using 8 dataloader workers\n",
      "Logging results to \u001b[1mruns/detect/train2\u001b[0m\n",
      "Starting training for 20 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       1/20      3.36G      2.304      1.766      1.081        122        640: 100%|██████████| 3588/3588 [06:24<00:00,  9.33it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 116/116 [00:23<00:00,  5.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1842      61736      0.462      0.268      0.264      0.118\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       2/20      3.94G      2.137      1.419      1.027        126        640: 100%|██████████| 3588/3588 [06:17<00:00,  9.52it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 116/116 [00:22<00:00,  5.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1842      61736      0.527      0.303      0.311      0.146\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       3/20      4.89G      2.073       1.36       1.01         55        640: 100%|██████████| 3588/3588 [06:13<00:00,  9.61it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 116/116 [00:22<00:00,  5.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1842      61736      0.561      0.327      0.339      0.163\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       4/20      4.89G      2.008      1.303     0.9982         83        640: 100%|██████████| 3588/3588 [04:19<00:00, 13.80it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 116/116 [00:08<00:00, 14.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1842      61736      0.563      0.341      0.357      0.175\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       5/20      4.89G      1.946      1.251     0.9876         44        640: 100%|██████████| 3588/3588 [03:29<00:00, 17.15it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 116/116 [00:08<00:00, 14.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1842      61736      0.591      0.354      0.373      0.184\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       6/20      4.89G      1.907      1.211     0.9775        602        640: 100%|██████████| 3588/3588 [03:28<00:00, 17.20it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 116/116 [00:08<00:00, 14.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1842      61736      0.598      0.359      0.382      0.189\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       7/20      4.89G      1.872      1.187     0.9727         70        640: 100%|██████████| 3588/3588 [03:31<00:00, 16.95it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 116/116 [00:08<00:00, 14.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1842      61736      0.611      0.368      0.395      0.198\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       8/20      4.89G      1.847      1.159     0.9642         87        640: 100%|██████████| 3588/3588 [03:31<00:00, 16.94it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 116/116 [00:08<00:00, 14.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1842      61736      0.618      0.377      0.403      0.205\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       9/20      4.89G      1.814       1.13      0.961        282        640: 100%|██████████| 3588/3588 [03:28<00:00, 17.17it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 116/116 [00:08<00:00, 13.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1842      61736       0.62      0.384      0.413      0.211\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      10/20      4.89G      1.797      1.116     0.9552        183        640: 100%|██████████| 3588/3588 [03:28<00:00, 17.17it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 116/116 [00:08<00:00, 13.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1842      61736      0.624      0.389      0.419      0.215\n",
      "Closing dataloader mosaic\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      11/20      4.89G      1.756      1.079     0.9413        107        640: 100%|██████████| 3588/3588 [03:22<00:00, 17.71it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 116/116 [00:08<00:00, 14.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1842      61736      0.641      0.388      0.423      0.217\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      12/20      4.89G      1.737      1.059     0.9372        129        640: 100%|██████████| 3588/3588 [03:24<00:00, 17.56it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 116/116 [00:08<00:00, 13.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1842      61736      0.635      0.391      0.425      0.219\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      13/20      4.89G      1.723      1.047     0.9339         70        640: 100%|██████████| 3588/3588 [03:23<00:00, 17.67it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 116/116 [00:08<00:00, 13.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1842      61736      0.645      0.394      0.431      0.223\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      14/20      4.89G      1.702      1.029     0.9312        101        640: 100%|██████████| 3588/3588 [03:22<00:00, 17.69it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 116/116 [00:08<00:00, 13.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1842      61736      0.637      0.399      0.433      0.224\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      15/20      4.89G      1.686       1.01     0.9281        153        640: 100%|██████████| 3588/3588 [03:22<00:00, 17.72it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 116/116 [00:08<00:00, 14.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1842      61736       0.65      0.399      0.437      0.227\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      16/20      4.89G      1.671      0.998     0.9257         43        640: 100%|██████████| 3588/3588 [03:22<00:00, 17.68it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 116/116 [00:08<00:00, 14.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1842      61736      0.653      0.401       0.44      0.229\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      17/20      4.89G       1.66     0.9862     0.9256         73        640: 100%|██████████| 3588/3588 [03:23<00:00, 17.67it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 116/116 [00:08<00:00, 14.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1842      61736      0.653      0.404      0.443      0.231\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      18/20      4.89G      1.652     0.9765     0.9213        121        640: 100%|██████████| 3588/3588 [03:22<00:00, 17.69it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 116/116 [00:08<00:00, 14.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1842      61736      0.651      0.407      0.444      0.232\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      19/20      4.89G      1.631     0.9583     0.9183        154        640: 100%|██████████| 3588/3588 [03:22<00:00, 17.73it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 116/116 [00:07<00:00, 14.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1842      61736      0.659      0.405      0.446      0.233\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      20/20      4.89G      1.621      0.949     0.9178        273        640: 100%|██████████| 3588/3588 [03:22<00:00, 17.69it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 116/116 [00:08<00:00, 13.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1842      61736       0.66      0.406      0.447      0.234\n",
      "\n",
      "20 epochs completed in 1.362 hours.\n",
      "Optimizer stripped from runs/detect/train2/weights/last.pt, 6.2MB\n",
      "Optimizer stripped from runs/detect/train2/weights/best.pt, 6.2MB\n",
      "\n",
      "Validating runs/detect/train2/weights/best.pt...\n",
      "Ultralytics 8.3.100 🚀 Python-3.10.13 torch-2.5.1 CUDA:0 (NVIDIA RTX A4000, 15977MiB)\n",
      "Model summary (fused): 72 layers, 3,005,843 parameters, 0 gradients\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 116/116 [00:09<00:00, 12.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1842      61736       0.66      0.406      0.447      0.234\n",
      "Speed: 0.1ms preprocess, 1.0ms inference, 0.0ms loss, 0.5ms postprocess per image\n",
      "Results saved to \u001b[1mruns/detect/train2\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "model = YOLO('yolov8n.pt')\n",
    "datapath=os.path.abspath(\"../datasets/ctw_and_textocr/data.yaml\")\n",
    "results = model.train(data=datapath, epochs=20, imgsz=640, device=0, batch=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training results evaluationa re auto-generated by YOLO and can be found on `runs` directory as default. Below are some important results to analyze."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `confusion_matrix.png` to check prediction/ground truth result matrix. For text detection there are only one class (`text`).  \n",
    "some important thing to note is the precision/recall to handle class imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(filename='runs/detect/train10/confusion_matrix.png', width=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `results.png` shows the plot of different metrics for each training epochs. The loss function used is `box_loss`.  \n",
    "Common evaluation metric is mAP which calculates area under Precision-Recall curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(filename='runs/detect/train10/results.png', width=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this validation loop we will try to validate our trained models.  \n",
    "Use the best checkpoint `best.pyt` for the training result in `runs` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = YOLO('runs/detect/train10/weights/best.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below initialize webcam and continuous inference, we can measure inference performance by showing the FPS counter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    if ret:\n",
    "        start = time.perf_counter()\n",
    "        \n",
    "        results = model(frame)\n",
    "        end = time.perf_counter()\n",
    "        \n",
    "        total_time = end-start\n",
    "        fps = 1/total_time\n",
    "        \n",
    "        \n",
    "        annotated_frame = results[0].plot()\n",
    "        \n",
    "        cv2.putText(annotated_frame, f\"FPS: {int(fps)}\", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "        cv2.imshow(\"YOLOv8 Inference\", annotated_frame)\n",
    "    \n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "        \n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can validate the model on our validation datasets using the code below, it will keep track of previous directory when training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = model.val()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ultralytics",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
